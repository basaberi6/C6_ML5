## ğŸ“¦ Models Folder â€“ README

This folder contains the notebooks and assets for training, evaluating, and analyzing machine learning models for predicting sales performance using the Superstore dataset.
03_model_training.ipynb is based on [nadeli11] model as the baseline and teammate_model.ipynb is based on [MuradAhmed00] that had the best performance.

---

### ğŸ“˜ 03_model_training.ipynb

#### ğŸ¯ Goal
Train regression models to predict `Sales` using engineered features from the cleaned Superstore dataset.

#### âœ… Key Tasks
- Load processed dataset with engineered features
- Split data into training and testing sets
- Train two models:
  - Linear Regression
  - Random Forest Regressor
- Evaluate performance using:
  - RÂ² Score
  - RMSE (Root Mean Squared Error)
  - MAE (Mean Absolute Error)
- Save trained models as `.pkl` files
- Log model metrics to `model_metrics_log.csv`

#### ğŸ“¤ Output

03_model_training.ipynb baseline model output
| Model             | RÂ²    | RMSE    | MAE     |
|------------------|-------|---------|---------|
| Linear Regression | 0.038 | 753.649 | 199.070 |
| Random Forest     | 0.591 | 491.690 | 85.937  |

ğŸ“ Models saved to: `Models/Artifacts/`  
ğŸ“„ Metrics logged to: `Models/model_metrics_log.csv`

---

### ğŸ“˜ 04_model_evaluation.ipynb

#### ğŸ¯ Goal
Compare the trained models using diagnostic and error analysis techniques.

#### âœ… Key Tasks
- Reload saved models
- Predict on the test set
- Generate visual diagnostics:
  - Residual histograms
  - Actual vs. Predicted scatter plots
  - Q-Q plots of residuals
  - Residuals vs. Predicted values
- Export worst-performing predictions (outliers)

#### ğŸ“Š Key Insights
- **Random Forest** produces tighter, more normally distributed residuals.
- **Linear Regression** shows greater error spread and more heteroscedasticity.
- Diagnostic plots confirmed the superiority of Random Forest for this dataset.

ğŸ“ Top 10 worst RF predictions exported to:  
`Models/Artifacts/top_rf_prediction_errors.csv`

---

### ğŸ“ Folder Structure

Models/
â”œâ”€â”€ 03_model_training.ipynb
â”œâ”€â”€ 04_model_evaluation.ipynb
â”œâ”€â”€ 05_hyperparamet_tuning.ipynb
â”œâ”€â”€ model_metrics_log.csv
â”œâ”€â”€ Artifacts/
â”‚ â”œâ”€â”€ random_forest_model.pkl
â”‚ â”œâ”€â”€ linear_regression_model.pkl
â”‚ â””â”€â”€ top_rf_prediction_errors.csv
### ğŸ“˜ 05_hyperparameter_tuning.ipynb

#### ğŸ¯ Goal
Optimize the Random Forest model using `GridSearchCV` to improve predictive accuracy on sales.

#### âœ… Key Tasks
- Load the feature-engineered dataset
- Use `GridSearchCV` with 5-fold cross-validation
- Tune hyperparameters:
  - `n_estimators`: [100, 200]
  - `max_depth`: [None, 10, 20]
  - `min_samples_split`: [2, 5]
  - `min_samples_leaf`: [1, 2]
- Evaluate the best estimator on the test set
- Save the optimized model and full tuning results

#### ğŸ“Š Visualization
A heatmap of cross-validated RÂ² scores was generated to show how model performance varies with tree depth and number of estimators.

#### ğŸ“¤ Output
- âœ… Best model: `Models/Artifacts/random_forest_tuned.pkl`
- ğŸ“ˆ GridSearch results: `Models/Artifacts/rf_gridsearch_results.csv`
